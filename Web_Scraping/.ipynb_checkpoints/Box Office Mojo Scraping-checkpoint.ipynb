{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclosure\n",
    "\n",
    "This script is provided purely for educational purposes. \n",
    "\n",
    "The following code was based on a script originally by csredino on Github. \n",
    "You can find his original code here: https://github.com/csredino/Box-Office-Mojo-Scrapper\n",
    "\n",
    "And now, a joke at my expense: \n",
    "![alt text](http://67.media.tumblr.com/2c9c31d17f47eb1d43fd5ea893360fc7/tumblr_o3nqj5hDzx1rn27pqo1_1280.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import httplib\n",
    "import socket\n",
    "import pickle\n",
    "\n",
    "# Defines a function to patch the httplib package\n",
    "# No, I don't completely understand how/why it works. I shamelessly copied this from Stack Overflow. Only thing I know is this: It works.\n",
    "def patch_http_response_read(func):\n",
    "    def inner(*args):\n",
    "        try:\n",
    "            return func(*args)\n",
    "        except httplib.IncompleteRead, e:\n",
    "            return e.partial\n",
    "        \n",
    "    return inner\n",
    "\n",
    "# Patches httplib package.\n",
    "httplib.HTTPResponse.read = patch_http_response_read(httplib.HTTPResponse.read)\n",
    "\n",
    "# Wraps the 'BeautifulSoup' function with a retry component. This is to counteract the occasional internet connection issue. \n",
    "# It won't solve all of the problems, but it'll make it so it doesn't have to restart quite so often. \n",
    "# That's part of the reason I rewrote the script so that it could be started at different index points. \n",
    "@retry()\n",
    "def BSoup(x):\n",
    "    print \"Souping...\" # Prints 'Souping...' to indicate work. Troubleshooting purposes.\n",
    "    r = BeautifulSoup(x) # Defines a variable to be returned from the function. \n",
    "    return(r) # Returns the result of the BeautifulSoup function. \n",
    "\n",
    "# Defines an 'alphabet' index, mostly for debugging purposes so I knew when/where the function was failing. \n",
    "alphabet= ['#','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "current_url=(\"http://www.boxofficemojo.com/movies/alphabetical.htm?letter=NUM&p=.html\")# starting point for search. \n",
    "movie_links = [] # Initializes movie_links as an empty list. \n",
    "\n",
    "soup = BeautifulSoup(urlopen(current_url).read())    # Pulls the content of the 'current_url' to be used for the following function.\n",
    "letters = soup.findAll('a', href= re.compile('letter=')) # Generate a list of links for the letter indices\n",
    "letter_index = [] # Initializes 'letter_index' as an empty list. \n",
    "for t in letters:\n",
    "    letter_index.append(\"http://www.boxofficemojo.com\" + t['href']) # Note: This will result in a list of 54 items, essentially listing the pages twice. \n",
    "for i in range(0,27): # I added this code to remove duplicates. \n",
    "    letter_index.pop()\n",
    "\n",
    "page_list2 = [] #Declared a second page_list index that wouldn't be reset in the loop. \n",
    "for n in range(0,27): # Loop through all letter indices for movies.\n",
    "    print time.ctime() # Prints the system time. Partially for debugging, partially for indicating that the script is still working.\n",
    "    print \"We are currently scraping alphabet index \" + str(alphabet[n]) # Prints index scrape for debugging purposes. \n",
    "    current_url = letter_index[n] # First level of segmentation, by letter index. We need to find the second level, by individual page within letter index, to sufficiently compartmentalize data collection.\n",
    "    print \"printing current_url\" # For debugging and letting the user know the script is still working. \n",
    "    print current_url\n",
    "    page_list2.append(current_url)\n",
    "    soup = BSoup(urlopen(current_url).read())\n",
    "    navbar = soup.find('div', 'alpha-nav-holder')\n",
    "    pages = navbar.findAll('a', href = re.compile('alphabetical'))\n",
    "    #print \"printing pages...\"\n",
    "    #print pages\n",
    "    page_list=[] # page_list is apparently reset for each letter. This is why I added the second index.\n",
    "    \n",
    "    if pages != None: #this only runs if there is a 2nd page for this letter\n",
    "        i = 0 # page list starts at 2 (consequence of page layout)\n",
    "        print len(page_list)\n",
    "        while i <len(page_list): # loop over multiple pages for each letter index\n",
    "            #print \"printing i\"\n",
    "            #print i\n",
    "            current_url=page_list[i]\n",
    "            #print \"printing current_url\"\n",
    "            #print current_url\n",
    "            soup = BSoup(urlopen(current_url).read())\n",
    "            movietable = soup.find('div',{'id':'main'})\n",
    "            #print \"printing movietable...\"\n",
    "            #print movietable\n",
    "            movies = movietable.findAll('a',href=re.compile('id='))\n",
    "            #print \"printing movies...\"\n",
    "            #print movies\n",
    "    #count1 = 1\n",
    "    for t in pages:\n",
    "        #print \"printing t...\"\n",
    "        #print t\n",
    "        #print \"printing t['href']\"\n",
    "        #print t['href']\n",
    "        #filename = \"pagelist\" + str(count1)\n",
    "        #print \"printing page list before\"\n",
    "        #print page_list\n",
    "        page_list.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "        page_list2.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "        #print \"printing page list after\"\n",
    "        #print page_list\n",
    "        #with open(filename,'w') as f:\n",
    "         #   pickle.dump(page_list,f)\n",
    "         #  f.close()\n",
    "        #count1 = count1+1\n",
    "    #print \"printing page list...\"\n",
    "    #print page_list\n",
    "    \n",
    "    movietable = soup.find('div',{'id':'main'})\n",
    "    movies = movietable.findAll('a', href=re.compile('id='))\n",
    "    for t in movies:\n",
    "        movie_links.append(\"http://www.boxofficemojo.com\"+t['href'])\n",
    "    \n",
    "    if pages != None: \n",
    "        i = 0\n",
    "        while i<len(page_list):\n",
    "            current_url = page_list[i]\n",
    "            print \"printing current url\"\n",
    "            print current_url\n",
    "            soup = BeautifulSoup(urlopen(current_url).read())\n",
    "            movietable = soup.find('div',{'id':'main'})\n",
    "            movies = movietable.findAll('a',href=re.compile('id='))\n",
    "            for t in movies:\n",
    "                movie_links.append(\"http://www.boxofficemojo.com\" + t['href'])\n",
    "            i+=1\n",
    "            #print \"printing movies\"\n",
    "            #print movies\n",
    "            #print \"printing movielinks\"\n",
    "            #print movie_links\n",
    "\n",
    "# I added the following code thinking that I would have to save and iterate through the links each time I wanted to restart.\n",
    "# That ended up not being the case, so the following code is a bit superfluous. You can comment it out if you'd like.\n",
    "f = open(\"movie_links\",'w') # Opens a file to save the movie_links list.\n",
    "pickle.dump(movie_links,f) # Dumps the contents of movie_links into a file.\n",
    "f.close() # closes the file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I rewrote the original script so that the following code was executed as an individual function instead of a looped script. \n",
    "# I was then able to write a loop that would pass the individual url and opened file for writing the scraped data. \n",
    "# Because of this, I could more easily see when an issue came up and then start it again without having to restart the whole process. \n",
    "\n",
    "def getmoviedata(url,open_file):\n",
    "    # The following block counteracts an indexing issue for two of the urls.\n",
    "    if \"elizabeth\" in url and \"elizabethtown\" not in url:\n",
    "        url = 'http://www.boxofficemojo.com/movies/?id=elizabeth%A0.htm'\n",
    "    if \"simpleplan\" in url:\n",
    "        url = 'http://www.boxofficemojo.com/movies/?id=simpleplan%A0.htm'\n",
    "    \n",
    "    # The following code prints the link and time being scanned so that the user knows where to look if issues arise. \n",
    "    print time.ctime()\n",
    "    print url\n",
    "    current_url = (url + \"&adjust_yr=2015&p=.htm\") \n",
    "    soup = BSoup(urlopen(current_url).read())\n",
    "    directors = soup.findAll('a',href=re.compile('Director&id'))\n",
    "    director_list=[]\n",
    "    for t in directors:\n",
    "        director_list.append(t.encode_contents())\n",
    "    for i in range(0,2):\n",
    "        if i>=len(director_list):\n",
    "            director_list.append('N/A')#fill rest of list\n",
    "    director1=director_list[0]\n",
    "    director2=director_list[1]\n",
    "\n",
    "    writers=soup.findAll('a', href= re.compile('Writer&id'))\n",
    "    writer_list=[]\n",
    "    for t in writers:\n",
    "        writer_list.append(t.encode_contents())\n",
    "    for i in range(0,2):\n",
    "        if i>=len(writer_list):\n",
    "            writer_list.append('N/A')\n",
    "    writer1=writer_list[0]\n",
    "    writer2=writer_list[1]\n",
    "\n",
    "    composers=soup.findAll('a', href= re.compile('Composer&id'))\n",
    "    composer_list=[]\n",
    "    for t in composers:\n",
    "        composer_list.append(t.encode_contents())\n",
    "    for i in range(0,2):\n",
    "        if i>=len(composer_list):\n",
    "            composer_list.append('N/A')\n",
    "    composer1=composer_list[0]\n",
    "    composer2=composer_list[1]\n",
    "\n",
    "    actors=soup.findAll('a', href= re.compile('Actor&id'))\n",
    "    actor_list=[]\n",
    "    for t in actors:\n",
    "        actor_list.append(t.encode_contents())\n",
    "    for i in range(0,6):\n",
    "        if i>=len(actor_list):\n",
    "            actor_list.append('N/A')\n",
    "    actor1=actor_list[0]\n",
    "    actor2=actor_list[1]\n",
    "    actor3=actor_list[2]\n",
    "    actor4=actor_list[3]\n",
    "    actor5=actor_list[4]\n",
    "    actor6=actor_list[5]\n",
    "\n",
    "    producers=soup.findAll('a', href= re.compile('Producer&id'))\n",
    "    producer_list=[]\n",
    "    for t in producers:\n",
    "        producer_list.append(t.encode_contents())\n",
    "    for i in range(0,6):\n",
    "        if i>=len(producer_list):\n",
    "            producer_list.append('N/A')\n",
    "    producer1=producer_list[0]\n",
    "    producer2=producer_list[1]\n",
    "    producer3=producer_list[2]\n",
    "    producer4=producer_list[3]\n",
    "    producer5=producer_list[4]\n",
    "    producer6=producer_list[5]\n",
    "\n",
    "\n",
    "    all_bs=soup.findAll('b')\n",
    "    b_list=[] #lots of the information we want is in bold, and appears in the same order on each page\n",
    "    for t in all_bs:\n",
    "        #print t\n",
    "        if 'Domestic Lifetime' not in t.encode_contents():#want to ignore the lifetime box office\n",
    "            b_list.append(t.encode_contents())\n",
    "    if len(b_list)>=10:#avoids bad entries with no box office data\n",
    "        if '$'in b_list[2] or 'n/a' in b_list[9]:#avoid movies w/o box office data, or unadjustable box office data, if not caught above\n",
    "            if 'n/a' in b_list[9]:#has a foreign release only, order is shifted\n",
    "                title=b_list[1]\n",
    "                domestic='N/A'\n",
    "                if 'N/A' not in b_list[2]:\n",
    "                    distributor=b_list[2].split('>')[1].split('<')[0]\n",
    "                else:\n",
    "                    distributor=b_list[2]\n",
    "                if len(b_list[3].split('>'))>3:#sometimes the release date is not in a hyperlink\n",
    "                    release=b_list[3].split('>')[2].split('<')[0]\n",
    "                else:\n",
    "                    release=b_list[3].split('>')[1].split('<')[0]\n",
    "                genre=b_list[4]\n",
    "                runtime=b_list[5]\n",
    "                rating=b_list[6]\n",
    "                budget=b_list[7]\n",
    "                worldwide=b_list[12]\n",
    "            else:\t#has a domestic release\n",
    "                title=b_list[1]\n",
    "                domestic=b_list[2]\n",
    "                if 'n/a' not in b_list[3]:\n",
    "                    distributor=b_list[3].split('>')[1].split('<')[0]\n",
    "                else:\n",
    "                    distributor=b_list[3]\n",
    "                if len(b_list[4].split('>'))>3:#sometimes the release date is not in a hyperlink\n",
    "                    release=b_list[4].split('>')[2].split('<')[0]\n",
    "                else:\n",
    "                    release=b_list[4].split('>')[1].split('<')[0]\n",
    "                genre=b_list[5]\n",
    "                runtime=b_list[6]\n",
    "                rating=b_list[7]\n",
    "                budget=b_list[8]\n",
    "                if len(b_list)==11 or '%' not in b_list[11]:#this means it only has a domestic release\n",
    "                    worldwide='N/A'\n",
    "                else:\n",
    "                    worldwide=b_list[13]\n",
    "            #print release\n",
    "            print time.ctime()\n",
    "            print \"writing...\"\n",
    "            open_file.writerow([title,director1,director2,domestic,distributor,release,genre,runtime,rating,budget,worldwide,actor1,actor2,actor3,actor4,actor5,actor6,producer1,producer2,producer3,producer4,producer5,producer6,writer1,writer2,composer1,composer2])#since this is in the big \"if\" it wont write to file if it is formated incorrectly\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code opens a csv file and prepares it for receiving the scraped data. \n",
    "f = open(\"moviedump.csv\",'wb')\n",
    "fieldnames = (\"title\",\"director1\",\"director2\",\"domestic\", \"distributor\",\"release\",\"genre\",\"runtime\",\"rating\",\"budget\",\"worldwide\",\"actor1\",\"actor2\",\"actor3\",\"actor4\",\"actor5\",\"actor6\",\"producer1\",\"producer2\",\"producer3\",\"producer4\",\"producer5\",\"producer6\",\"writer1\",\"writer2\",\"composer1\",\"composer2\")\n",
    "output = csv.writer(f, delimiter=\",\")\n",
    "output.writerow(fieldnames)\n",
    "\n",
    "# I set up the following code so that the csv file wouldn't be closed unless the scrape process was complete. \n",
    "# In the case of an error, the script will return a message indicating at what index point the script failed. \n",
    "# The user can then continue the process by executing the script in the following cell. \n",
    "progress = 0\n",
    "completion = len(movie_links)\n",
    "Executed = False\n",
    "Error = False\n",
    "while Executed == False:\n",
    "    # The following 'if' conditionals are designed to skip bad links I have been unable to rectify. \n",
    "    # It's possible that these pages are just temporarily disabled or malfunctioning. \n",
    "    if progress == 1478:\n",
    "        progress+=1\n",
    "    if progress == 3313:\n",
    "        progress+=1\n",
    "    if progress == 5410:\n",
    "        progress+=1\n",
    "    if progress == 8728:\n",
    "        progress+=1\n",
    "    if progress == 8730:\n",
    "        progress +=1\n",
    "    if progress == 10921:\n",
    "        progress +=1\n",
    "    try:\n",
    "        getmoviedata(movie_links[progress],output)\n",
    "    except:\n",
    "        Executed = True\n",
    "        Error = True\n",
    "    \n",
    "    if Executed == False:\n",
    "        progress+=1\n",
    "    if progress >= completion:\n",
    "        Executed = True\n",
    "\n",
    "\n",
    "if Error == False:\n",
    "    output.close()\n",
    "    print \"Data Scraped Successfully\"\n",
    "\n",
    "else:\n",
    "    errorpoint = progress\n",
    "    print \"Data scrape interrupted at index \", str(errorpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since the 'progress' variable will not be changed in the case of an error, the user can just run the current cell without any changes.\n",
    "\n",
    "Executed = False\n",
    "Error = False\n",
    "while Executed == False:\n",
    "    if progress == 1478:\n",
    "        progress+=1\n",
    "    if progress == 3313:\n",
    "        progress+=1\n",
    "    if progress == 5410:\n",
    "        progress+=1\n",
    "    if progress == 8728:\n",
    "        progress+=1\n",
    "    if progress == 8730:\n",
    "        progress +=1\n",
    "    if progress == 10921:\n",
    "        progress +=1\n",
    "    try:\n",
    "        getmoviedata(movie_links[progress],output)\n",
    "    except:\n",
    "        Executed = True\n",
    "        Error = True\n",
    "    \n",
    "    if Executed == False:\n",
    "        progress+=1\n",
    "    if progress >= completion:\n",
    "        Executed = True\n",
    "\n",
    "\n",
    "if Error == False:\n",
    "    output.close()\n",
    "    print \"Data Scraped Successfully\"\n",
    "\n",
    "else:\n",
    "    errorpoint = progress\n",
    "    print \"Data scrape interrupted at index \", str(errorpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
